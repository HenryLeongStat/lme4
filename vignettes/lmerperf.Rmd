---
title: "lmer Performance tips"
---

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{lmer Performance Tips}
-->

```{r opts, echo = FALSE, message = FALSE}
library("knitr")
knitr::opts_chunk$set(
  )
load(system.file("testdata", "lmerperf.rda", package="lme4"))
```

```{r loadpkg,message=FALSE}
library("lme4")
```

## overview

In general `lme4`'s algorithms scale reasonably well with the number of observations
and the number of random effect levels. The biggest bottleneck is in the number of
*top-level parameters*, i.e. covariance parameters for
`lmer` fits or `glmer` fits with `nAGQ`=0 [`length(getME(model, "theta"))`], 
covariance and fixed-effect parameters for `glmer` fits with `nAGQ`>0. `lme4` does a derivative-free
(by default) nonlinear optimization step over the top-level parameters.

`lme4` automatically constructs the random effects model matrix ($Z$) as a sparse matrix.
At present it does *not* allow an option for a sparse fixed-effects model matrix ($X$), which
is useful if the fixed-effect model includes factors with many levels. Treating such factors
as random effects instead, and using the modular framework (`?modular`) to fix the variance
of this random effect at a large value, will allow it to be modeled using a sparse matrix.
(The estimates will converge to the fixed-effect case in the limit as the variance goes to infinity.)

## setting `calc.derivs = FALSE`

After finding the best-fit model parameters (in most cases using *derivative-free*
algorithms such as Powell's BOBYQA or Nelder-Mead, `[g]lmer` does a series of finite-difference
calculations to estimate the gradient and Hessian at the MLE. These are used to
try to establish whether the model has converged reliably, and (for `glmer`) to
estimate the standard deviations of the fixed effect parameters (a less accurate
approximation is used if the Hessian estimate is not available. As currently implemented, this computation takes `2*n^2 -  n + 1` additional evaluations of the deviance,
where `n` is the total number of top-level parameters. Using  `control = [g]lmerControl(calc.derivs = FALSE)` to turn off this calculation can speed up 
the fit, e.g.

```{r noderivs,eval=FALSE}
m0 <- lmer(y ~ service * dept + (1|s) + (1|d), InstEval,
     control = lmerControl(calc.derivs = FALSE))
```

Benchmark results for this run with and without derivatives show an
approximately 16% speedup. This is a case with only 2 top-level
parameters, but the fit took only 31 deviance function evaluations
(see `m0@optinfo$feval`) to converge, so the effect of the additional 7 ($n^2 -n +1$)
function evaluations is noticeable.

```{r bench, echo=FALSE}
b
```

## initial conditions

- models that only contain random effects of the form `(1|f)` use better 
  starting values for the optimization which in tests have cut run time in 
  certain examples by up to 50% relative to the previous default starting 
  values.  The `InstEval` fit shown above is one such example.
  
- 

- `lmer` uses the `bobyqa` optimizer from the `minqa` package by default;
  `glmer` uses a combination of Nelder-Mead and `bobyqa`.  If you
  are specifying the `optimx` package optimizer, note that by 
  default `optimx` performs 
  certain time-consuming processing at the beginning and end which can be 
  turned off as follows (here we have specified the `"nlminb"` method but this 
  applies to any `optimx` method):

```{r nlminbfit,eval=FALSE}
library("optimx")
lmer(y ~ service * dept + (1|s) + (1|d), InstEval,
     control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
     optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE)))
```

- the `nloptr` package supports a variety of algorithms and importantly 
  supports additional stopping criteria which can stop the optimization
  earlier if it believes it has reached the optimum.
  For many problems using these stopping 
  criteria will result in the 
  same solution or nearly the same solution as the default optimizer but in
  less time (up to 50 percent savings have been observed); however, in
  some cases it may stop prematurely giving suboptimal results.
  (In the example below omit `print_level` if output tracing is not desired and 
  increase `maxeval` if the optimization requires more than 1000 iterations
  and you wish to allow it to proceed.)

```{r nlopt,eval=FALSE}
nlopt <- function(par, fn, lower, upper, control) {
    .nloptr <<- res <- nloptr(par, fn, lb = lower, ub = upper, 
        opts = list(algorithm = "NLOPT_LN_BOBYQA", print_level = 1,
        maxeval = 1000, xtol_abs = 1e-6, ftol_abs = 1e-6))
    list(par = res$solution,
         fval = res$objective,
         conv = if (res$status > 0) 0 else res$status,
         message = res$message
    )
}
lmer(y ~ service * dept + (1|s) + (1|d), InstEval,
    control = lmerControl(optimizer = "nloptwrap", calc.derivs = FALSE))
```
